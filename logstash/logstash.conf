# ===================================================================================
# logstash.conf
#
# Logstash pipeline for ingesting logs from Filebeat
# 1) Input  - Receives logs from Beats (Filebeat) on port 5044
# 2) Filter - Grok parsing, timestamp processing, message cleanup
# 3) Output - Sends logs to Elasticsearch and optionally to console
# ===================================================================================

# --------------------------------- Input Plugin ----------------------------------
input {
  beats {
    port => 5044                                # Listen for Beats on TCP port 5044
    # ssl => true                               # Enable if TLS is configured
    # ssl_certificate => "/etc/logstash/certs/logstash.crt"
    # ssl_key => "/etc/logstash/certs/logstash.key"
  }
}

# --------------------------------- Filter Plugin ---------------------------------
filter {
  # --- GROK: Parse timestamp, log level, service, trace ID, span ID, message ---
  grok {
    match => {
      "message" =>
        "^%{TIMESTAMP_ISO8601:log_timestamp} \[%{LOGLEVEL:log_level}\] service=%{WORD:service} trace_id=%{NOTSPACE:trace_id} span_id=%{NOTSPACE:span_id} %{GREEDYDATA:rest_of_message}"
    }
    tag_on_failure => ["_grokparsefailure"]     # Add tag if parsing fails
  }

  # --- DATE: Convert 'log_timestamp' to Logstash @timestamp ---
  date {
    match => ["log_timestamp", "yyyy-MM-dd HH:mm:ss,SSS"]
    timezone => "Asia/Kolkata"                  # Set timezone for timestamp parsing
    tag_on_failure => ["_dateparsefailure"]
  }

  # --- RENAME: Use parsed message only if grok succeeded ---
  if "_grokparsefailure" not in [tags] {
    mutate {
      rename => { "rest_of_message" => "message" }
    }
  } else {
    mutate {
      add_tag => ["unparsed_log"]               # Tag log lines that failed grok
    }
  }

  # --- (Optional) Docker Metadata Enrichment ---
  # docker_metadata {
  #   host => "unix:///var/run/docker.sock"
  # }

  # --- (Optional) Remove unused fields ---
  # mutate { remove_field => ["log_timestamp"] }
}

# --------------------------- Output Plugin: Route to Indices ---------------------------
output {
  # --- Conditional Output: Route logs based on input or tag ---
  # Route container logs to `containers-*`
  if "container_log" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]        # Elasticsearch host
      index => "containers-%{+YYYY.MM.dd}"          # Daily index for container logs
    }
  }
  # Route app logs to `techsavvyrc-*`
  else if "app_log" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "techsavvyrc-%{+YYYY.MM.dd}"         # Daily index for application logs
    }
  }
  # Fallback for uncategorized logs â†’ `unknown-*`
  else {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "unknown-%{+YYYY.MM.dd}"             # Index for unidentified logs
    }
  }

  # Output all events to console for debugging
  stdout {
    codec => rubydebug                              # Pretty-print all events
  }

  # --- (Optional) Output only app logs ---
  # elasticsearch {
  #   hosts => ["http://elasticsearch:9200"]
  #   index => "techsavvyrc-%{+YYYY.MM.dd}"         # Use only for app logs
  # }
}
